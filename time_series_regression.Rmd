---
title: "Intro to Time Series Regression"
author: "William Chiu"
date: "3/10/2022"
output:
  powerpoint_presentation:
    reference_doc: william_template.pptx
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

options(digits=3)

library(tidyverse)
library(broom)
library(broom.mixed)
library(np)
library(nlme)
library(lmtest)
library(sandwich)
library(purrr)
library(furrr)
library(ModelMetrics)

future::plan(multisession, workers=2)

Ksim <- 25000
boot_samp <- 1000
```

## About Me

-   MSIA Class of 2013
-   Works for a bank
-   Team develops models to explain and forecast:
    -   Credit Losses
    -   Loan Prepayments
    -   Mortgage Rates
    -   Deposit Attrition
-   Primarily uses SQL and R

## Time Series Regression

-   Time series is data collected in equally-spaced time intervals.

-   Linear regression estimates the linear relationship between a continuous response (`y`) and one or more predictors (`x`).

-   Ordinary least squares (OLS) is the most common implementation of linear regression, and estimates the coefficients that minimize the error sum of squares:

$ESS = \sum_{i=1}^{n}{\left ( y_i - (\widehat{\beta_0} + \widehat{\beta_1}X_{1,i} + ... + \widehat{\beta_p}X_{p,i}) \right )^2}$

-   Using linear regression on time series is called time series regression.

## OLS Assumptions for Time Series

-   Population data generating process (dgp) most suitable for OLS:

<br>

$y_t = \beta_0 + \beta_1X_{1,t} + ... + \beta_pX_{p,t} + \varepsilon_t$

$\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$

<br>

-   Linear parameters

-   Independent errors (no serial correlation)

-   Normally distributed errors

-   Equal variance errors (homoscedasticity)

-   Stationary response and predictors (unless there is co-integration)

## Violation: Serial Correlation

-   Unfortunately, the data generating process (dgp) for time series often violates the independent errors assumption. For example:

<br>

$y_t = \beta_0 + \beta_1X_{1,t} + ... + \beta_pX_{p,t} + \varepsilon_t$

$\varepsilon_t = \rho\varepsilon_{t-1} + v_t$

$v_t \sim \mathcal{N}(0, \sigma^2)$

<br>

-   If $\rho=0$, then there is no serial correlation.

-   If $-1<\rho<1$, then there is serial correlation.

## Simulate predictors with serial correlation

```{r, echo=TRUE}
set.seed(2013) 

x1 <- arima.sim(list(order = c(1,0,0), ar = 0.6), n = 200, sd=2, mean=2)
x2 <- arima.sim(list(order = c(1,0,0), ar = -0.6), n = 200, sd=3, mean=2)
x3 <- arima.sim(list(order = c(1,0,0), ar = 0.6), n = 200, sd=4, mean=2)
```

## Simulate response with serial correlation

```{r, echo=TRUE}
epsilon <- arima.sim(list(order = c(1,0,0), ar = 0.6), n = 200, sd=5, mean=0)

y <- 10 + 3.5*x1 - 1.5*x2 + epsilon 
## x3 is not part of the dgp

df <- data.frame(y=y, x1=x1, x2=x2, x3=x3, t=1:length(y))
```

## Plot simulated data (Train)

```{r, warning=FALSE, message=FALSE, fig.width = 14,fig.height = 7}

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#000000", "#F0E442",
                "#0072B2", "#D55E00", "#CC79A7")

df_plot <- df %>% 
  slice_head(n=150) %>% # first 150 obs
  pivot_longer(-t, "variable", "value") %>% 
  arrange(desc(variable), t) # preserve time order

ggplot(df_plot, aes(x=t, y=value, color=variable)) +
  geom_line(size=2) +
  theme_bw(base_size=28) +
  scale_colour_manual(values=cbbPalette)

```

## Split between train and test

```{r, echo=TRUE}
train <- head(df, 150)
test <- tail(df, 50)
```

## OLS ignores serial correlation

::: columns
::: column
-   Under serial correlation, OLS parameter estimates are unbiased and consistent
-   However, standard errors are incorrect
-   And p-values are incorrect
-   `x3` should *not* be statistically significant
:::

::: column
```{r}

lm_mod <- lm(y ~ x1 + x2 + x3, data=train)

knitr::kable(tidy(lm_mod))
```
:::
:::

## Detecting serial correlation

::: columns
::: column
-   Durbin-Watson test is a popular test for serial correlation, but suffers from some major drawbacks.

-   Breusch-Godfrey Test is a better test.
:::

::: column
```{r}
knitr::kable(tidy(bgtest(lm_mod)), digits=28, caption="Null Hypothesis: No serial correlation")
```
:::
:::

## Remediating serial correlation

-   Replace OLS with GLS
-   Keep OLS but estimate robust (HAC) standard errors
-   Keep OLS but bootstrap the standard errors

## Generalized Least Squares (Cochrane--Orcutt)

::: columns
::: column
1.  Estimate $\rho$
2.  Transform each response and predictor into "partial differences" (e.g., $y_t - \widehat{\rho}y_{t-1}$)
3.  Estimate the transformed model using OLS
4.  Re-calculate the coefficient estimates in terms of the original training data
:::

::: column
```{r, message=FALSE, warning=FALSE}

gls_mod <- gls(y ~ x1 + x2 + x3, data=train, correlation=corARMA(p=1, q=0), method="ML")

knitr::kable(tidy(gls_mod))
```
:::
:::

## Newey-West HAC Correction

::: columns
::: column
-   Keep OLS but estimate robust (HAC) standard errors
:::

::: column
```{r, message=FALSE, warning=FALSE}
nw <- coeftest(lm_mod, vcov=NeweyWest)

knitr::kable(tidy(nw ## Newey-West correction
))
```
:::
:::

## Andrews HAC Correction

::: columns
::: column
-   Keep OLS but estimate robust (HAC) standard errors
:::

::: column
```{r, message=FALSE, warning=FALSE}
andrews <- coeftest(lm_mod, vcov=kernHAC)

knitr::kable(tidy(andrews ## Andrews correction
))

```
:::
:::

## Block Bootstrap

::: columns
::: column
-   Keep the OLS coefficient estimates but *bootstrap* the standard errors
-   Break the time series into sequential blocks (non-random)
-   Randomly sample blocks with replacement
-   Unfortunately, results are sensitive to block size
:::

::: column
```{r, message=FALSE, warning=FALSE}
even_blocksize <- which(nrow(train) %% c(1:nrow(train))==0)

optimal_blocksize <- max(b.star(lm_mod$residuals))
    
optimal_blocksize <- even_blocksize[which.min(abs(even_blocksize - optimal_blocksize))]

nbrClusters <- nrow(train)/optimal_blocksize

clusterID <- rep(seq(nbrClusters),each=optimal_blocksize)

set.seed(1)

blockboot <- coeftest(lm_mod, vcov=vcovBS, cluster=clusterID, R=boot_samp)
knitr::kable(tidy(blockboot
))
```
:::
:::

## Test Set Performance (Ex-post, OLS vs. GLS)

```{r, fig.width = 14,fig.height = 7}
ols_pred <- predict(lm_mod, newdata=test)
gls_pred <- predict(gls_mod, newdata=test)

test_plot <- test

test_plot$ols <- ols_pred
test_plot$gls <- gls_pred

test_plot <- test_plot %>% 
  select(t, y, ols, gls) %>% 
  pivot_longer(-t, "variable", "value") %>% 
  arrange(variable, t)

cbbPalette2 <- c("#E69F00", "#56B4E9", "#000000", "#009E73", "#F0E442",
                 "#0072B2", "#D55E00", "#CC79A7")

ggplot(test_plot, aes(x=t, y=value, color=variable, lty=variable)) +
  geom_line(size=1.5) +
  theme_bw(base_size=28) +
  scale_colour_manual(values=cbbPalette2) +
  scale_linetype_manual(values=c("dotted","dashed","solid"))
```

## Inference is harder than prediction

-   Test set performance appears similar between OLS and GLS

-   However, the p-values for `x3` differ substantially by method: `lm`, `gls`, `newey-west`, `andrews`, `bootstrap`.

-   The p-values from simulated data are *sensitive* to the random seeds

-   In order to measure the quality of the methods:

    1.  Re-simulate the training data ($K = `r Ksim`$) with varying seeds and $\rho$
    2.  Measure how often Type 1 errors occur ($\alpha=0.10$)
    3.  Measure test set error (i.e., average MSE over K simulations, then take the square root)

-   Since $\beta_3 = 0$ in the simulation, any p-values \<= $\alpha$ is a Type 1 Error

## Type 1 Error Frequency

::: columns
::: column
```{r, message=FALSE, warning=FALSE}
doOneSimulation <- function(randomSeed=2013, alpha=0.10, rho=0.6) {
  set.seed(randomSeed) 
  
  x1 <- arima.sim(list(order = c(1,0,0), ar = rho), n = 200, sd=2, mean=2)
  x2 <- arima.sim(list(order = c(1,0,0), ar = rho*-1), n = 200, sd=3, mean=2)
  x3 <- arima.sim(list(order = c(1,0,0), ar = rho), n = 200, sd=4, mean=2)
  
  epsilon <- arima.sim(list(order = c(1,0,0), ar = rho), n = 200, sd=5, mean=0)
  
  y <- 10 + 3.5*x1 - 1.5*x2 + epsilon 
  ## x3 is not part of the dgp
  
  df <- data.frame(y=y, x1=x1, x2=x2, x3=x3, t=1:length(y))
  
  train <- head(df, 150)
  test <- tail(df, 50)
  
  lm_mod <- lm(y ~ x1 + x2 + x3, data=train)
  
  gls_mod <- gls(y ~ x1 + x2 + x3, data=train, correlation=corARMA(p=1, q=0), method="ML")
  
  nw <- coeftest(lm_mod, vcov=NeweyWest)
  
  andrews <- coeftest(lm_mod, vcov=kernHAC)
  
  even_blocksize <- which(nrow(train) %% c(1:nrow(train))==0)
  
  optimal_blocksize <- max(b.star(lm_mod$residuals))
      
  optimal_blocksize <- even_blocksize[which.min(abs(even_blocksize - optimal_blocksize))]
  
  nbrClusters <- nrow(train)/optimal_blocksize
  
  clusterID <- rep(seq(nbrClusters),each=optimal_blocksize)
  
  set.seed(1)
  
  blockboot <- coeftest(lm_mod, vcov=vcovBS, cluster=clusterID, R=boot_samp)
  
  all_models <- list(lm=lm_mod, gls=gls_mod, nw=nw, andrews=andrews, bootstrap=blockboot)
  
  df_of_models <- map_dfr(all_models, tidy, .id="method")
  
  out <- df_of_models %>% 
    filter(term == "x3") %>% 
    mutate(Type1ErrorFlag = `p.value` <= alpha)
  
  ## test set performance
  
  ols_pred <- predict(lm_mod, newdata=test)
  gls_pred <- predict(gls_mod, newdata=test)
  
  test_plot <- test
  
  test_plot$ols <- ols_pred
  test_plot$gls <- gls_pred
  
  test_mse_lm <- data.frame(method='lm', Test_MSE = mse(actual=test_plot$y, predicted=test_plot$ols),
                            Train_MSE = mse(actual=train$y, predicted=lm_mod$fitted.values))
  test_mse_gls <- data.frame(method='gls', Test_MSE = mse(actual=test_plot$y, predicted=test_plot$gls),
                             Train_MSE = mse(actual=train$y, predicted=gls_mod$fitted))
  
  test_mse <- bind_rows(test_mse_lm, test_mse_gls)
  
  ## merge results
  
  out <- out %>% 
    left_join(test_mse, by="method")

  return(out)
}

a <- Sys.time()

allSimulations <- map_dfr(c(`1-Low (0.1)`=0.1, `2-Moderate (0.7)`=0.7, `3-Very High (0.9)`=0.9),
                          ~ future_map_dfr(1:Ksim, doOneSimulation, rho=.x, .options =
                                             furrr_options(packages=c("broom", "broom.mixed"),
                                                           seed=NULL)),
                          .id="rho")

b <- Sys.time()
c <- difftime(b, a, units="mins")

future:::ClusterRegistry("stop")

tbl <- allSimulations %>% 
  group_by(rho, method) %>% 
  summarize(Prob_Type_1_Error = mean(Type1ErrorFlag),
            Avg_Train_RMSE = sqrt(mean(Train_MSE)),
            Avg_Test_RMSE = sqrt(mean(Test_MSE)),
            .groups = 'drop') %>% 
  arrange(rho, Prob_Type_1_Error) 

knitr::kable(head(tbl,5), digits=3)
```
:::

::: column
```{r}
knitr::kable(tbl[6:10,], digits=3)
```
:::
:::

## Very High Serial Correlation

::: columns
::: column
-   Under very high serial correlation, most of the previous methods struggle to return the correct p-values.

-   If $\widehat{\rho}$ is close to -1 or 1, then check for non-stationary residuals (e.g., Phillips-Ouliaris Cointegration Test; Pesaran-Shin-Smith Cointegration Test).

-   Non-stationary residuals could be a strong sign of spurious regression
:::

::: column
```{r}
knitr::kable(tail(tbl,5), digits=3)

```
:::
:::

## Serial Correlation and Test Set RMSE

$$RMSE_{test} = \sqrt[ ]{\sigma_{\varepsilon}^2} = \sqrt[ ]{ \frac{\sigma_v^2}{1- \rho^2} }$$

<br>

The Pennsylvania State University. (2018). 10.3 - Regression with Autoregressive Errors. Applied Regression Analysis. Retrieved February 27, 2022, from <https://online.stat.psu.edu/stat462/node/189/>

## Violation: Non-stationary response and predictors

-   The data generating process (dgp) for time series is often non-stationary. For example, $z_t$ and $w_t$ are independent random walks:

$$z_t = z_{t-1} + v_t$$ $$v_t \sim \mathcal{N}(0, \sigma_v^2)$$ $$w_t = w_{t-1} + e_t$$ $$e_t \sim \mathcal{N}(c, \sigma_e^2)$$

<br>

-   When the response and predictors are non-stationary, OLS and GLS could be biased and inconsistent (i.e., spurious) -- except in the case of cointegration

-   Non-stationary errors adversely affect *both* prediction and inference (while serially correlated errors affect only inference)

## Simulate non-stationary predictors

```{r, echo=TRUE}
set.seed(2013) 

w1 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=2, mean=2)
w2 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=3, mean=2)
w3 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=4, mean=2)
```

## Simulate non-stationary response

-   Suppose $z_t$ is a random walk and does **not** depend on any predictors

$$z_t = z_{t-1} + v_t$$ $$v_t \sim \mathcal{N}(0, \sigma^2)$$

-   The predictors $w_1$, $w_2$, $w_3$ should not affect $z_t$

<br>

```{r, echo=TRUE}
z <- arima.sim(list(order = c(0,1,0)), n = 200, sd=5, mean=0)

## w1, w2, w3 are not part of the dgp (and z is a random walk)

df_nonstationary <- data.frame(z=z, w1=w1, w2=w2, w3=w3, t=1:length(z))
```

## Plot simulated data (Train)

```{r, warning=FALSE, message=FALSE, fig.width = 14,fig.height = 7}

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#000000", "#F0E442",
                "#0072B2", "#D55E00", "#CC79A7")

df_plot_ns <- df_nonstationary %>% 
  slice_head(n=150) %>% # first 150 obs
  pivot_longer(-t, "variable", "value") %>% 
  arrange(desc(variable), t) # preserve time order

ggplot(df_plot_ns, aes(x=t, y=value, color=variable)) +
  geom_line(size=2) +
  theme_bw(base_size=28) +
  scale_colour_manual(values=cbbPalette) +
  facet_wrap(variable ~ ., scales="free_y") +
  theme(legend.position = "None")

```

## Split between train and test

```{r, echo=TRUE}
train_ns <- head(df_nonstationary, 150)
test_ns <- tail(df_nonstationary, 52) # helps with plotting
```

## OLS ignores non-stationarity

::: columns
::: column
```{r}
lm_mod_ns <- lm(z ~ w1 + w2 + w3, data=train_ns)

knitr::kable(tidy(lm_mod_ns))
```
:::

::: column
```{r}
knitr::kable(glance(lm_mod_ns)[,1:2])
```
:::
:::

## Test Set Error Explodes

```{r, fig.width = 14,fig.height = 7}
gls_mod_ns <- gls(z ~ w1 + w2 + w3, data=train_ns, correlation=corARMA(p=1, q=0), method="ML")

ols_pred_ns <- predict(lm_mod_ns, newdata=test_ns)
gls_pred_ns <- predict(gls_mod_ns, newdata=test_ns)
rw <- rep(tail(train_ns$z,1), nrow(test_ns))

test_plot_ns <- test_ns

test_plot_ns$ols <- ols_pred_ns
test_plot_ns$gls <- gls_pred_ns
test_plot_ns$rw <- rw

test_plot_ns <- test_plot_ns %>% 
  select(t, z, ols, gls, rw) %>% 
  pivot_longer(-t, "variable", "value") %>% 
  arrange(variable, t)

cbbPalette2 <- c("#E69F00", "#56B4E9", "#009E73", "#000000", "#F0E442",
                 "#0072B2", "#D55E00", "#CC79A7")

ggplot(test_plot_ns, aes(x=t, y=value, color=variable, lty=variable)) +
  geom_line(size=1.5) +
  theme_bw(base_size=28) +
  scale_colour_manual(values=cbbPalette2) +
  scale_linetype_manual(values=c("dotted","dashed","dotted", "solid"))
```

## Non-stationarity is a very big problem

::: columns
::: column
-   Simulate $K = `r Ksim`$ data sets where the response and predictors are independent random walks (non-stationary)

-   OLS inferences are highly unreliable

-   OLS and GLS both perform worse than a naive model (i.e., take the last response from the training set and assume the response never changes in the test set)
:::

::: column
```{r}
doOneSimulationNS <- function(randomSeed=2013, alpha=0.10) {
  set.seed(randomSeed) 
  
  w1 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=2, mean=2)
  w2 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=3, mean=2)
  w3 <- arima.sim(list(order = c(0,1,0)), n = 200, sd=4, mean=2)
  
  z <- arima.sim(list(order = c(0,1,0)), n = 200, sd=5, mean=0)
  
  ## w1, w2, w3 are not part of the dgp (and z is a random walk)
  
  df_nonstationary <- data.frame(z=z, w1=w1, w2=w2, w3=w3, t=1:length(z))
  
  trainNS <- head(df_nonstationary, 150)
  testNS <- tail(df_nonstationary, 50)
  
  lm_mod_ns <- lm(z ~ w1 + w2 + w3, data=trainNS)
    
  gls_mod_ns <- gls(z ~ w1 + w2 + w3, data=trainNS, correlation=corARMA(p=1, q=0), method="ML")

  nw <- coeftest(lm_mod_ns, vcov=NeweyWest)
  
  andrews <- coeftest(lm_mod_ns, vcov=kernHAC)
  
  even_blocksize <- which(nrow(trainNS) %% c(1:nrow(trainNS))==0)
  
  optimal_blocksize <- max(b.star(lm_mod_ns$residuals))
      
  optimal_blocksize <- even_blocksize[which.min(abs(even_blocksize - optimal_blocksize))]
  
  nbrClusters <- nrow(trainNS)/optimal_blocksize
  
  clusterIDns <- rep(seq(nbrClusters),each=optimal_blocksize)
  
  set.seed(1)
  
  blockboot <- coeftest(lm_mod_ns, vcov=vcovBS, cluster=clusterIDns, R=boot_samp)
  
  all_models <- list(lm=lm_mod_ns, gls=gls_mod_ns, nw=nw, andrews=andrews, bootstrap=blockboot)
  
  df_of_models <- map_dfr(all_models, tidy, .id="method")
  
  out <- df_of_models %>% 
    filter(term == "w3") %>% 
    mutate(Type1ErrorFlag = `p.value` <= alpha)
  
  rw_df <- data.frame(method="rw", Type1ErrorFlag=0)
  
  out <- bind_rows(out, rw_df)
  
  ## test set performance
  
  ols_pred <- predict(lm_mod_ns, newdata=testNS)
  gls_pred <- predict(gls_mod_ns, newdata=testNS)
  rw_pred <- rep(tail(trainNS$z,1), nrow(testNS))
  
  test_plot <- testNS
  
  test_plot$ols <- ols_pred
  test_plot$gls <- gls_pred
  test_plot$rw <- rw_pred
  
  test_mse_lm <- data.frame(method='lm', Test_MSE = mse(actual=test_plot$z, predicted=test_plot$ols),
                            Train_MSE = mse(actual=trainNS$z, lm_mod_ns$fitted.values))
  
  test_mse_gls <- data.frame(method='gls', Test_MSE = mse(actual=test_plot$z, predicted=test_plot$gls),
                             Train_MSE = mse(actual=trainNS$z, predicted=gls_mod_ns$fitted))
  
  test_mse_rw <- data.frame(method='rw', Test_MSE = mse(actual=test_plot$z, predicted=test_plot$rw),
                            Train_MSE=NA)
  
  test_mse <- bind_rows(test_mse_lm, test_mse_gls, test_mse_rw)
  
  ## merge results
  
  out <- out %>% 
    left_join(test_mse, by="method")

  return(out)
}

a <- Sys.time()

allSimulationsNS <- future_map_dfr(1:Ksim, doOneSimulationNS, .options =
                                             furrr_options(packages=c("broom", "broom.mixed"),
                                                           seed=NULL))

b <- Sys.time()
c <- difftime(b, a, units="mins")

future:::ClusterRegistry("stop")

tblNS <- allSimulationsNS %>% 
  group_by(method) %>% 
  summarize(Prob_Type_1_Error = mean(Type1ErrorFlag),
            Avg_Train_RMSE = sqrt(mean(Train_MSE)),
            Avg_Test_RMSE = sqrt(mean(Test_MSE)),
            .groups = 'drop') %>% 
  arrange(Prob_Type_1_Error) 

knitr::kable(tblNS, digits=3)
```
:::
:::

## Remediating non-stationarity

1.  Transform each response and/or predictor into "first differences" (e.g., $y_t - y_{t-1}$)
2.  Estimate the transformed model using OLS or GLS
3.  Forecast with the last historical value and the cumulative sum of the predicted first differences:

$\widehat{y_{t+h}} = y_t + \sum_{i=t+1}^{t+h}{\widehat{\Delta{y_i}}}$

## Conclusions

-   OLS is most suitable for independent errors

-   The independent error assumption is often violated with time series regression

-   Under serial correlation, standard errors and p-values from OLS are unreliable

-   Fortunately, OLS coefficient estimates remain unbiased and consistent

-   Remediating serial correlation:

    -   GLS
    -   OLS with HAC standard errors
    -   OLS with block bootstrapped standard errors

-   Under non-stationary errors, beware of spurious regressions
